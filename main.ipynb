{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T05:18:26.349758Z",
     "iopub.status.busy": "2022-01-15T05:18:26.349029Z",
     "iopub.status.idle": "2022-01-15T05:18:32.371958Z",
     "shell.execute_reply": "2022-01-15T05:18:32.370941Z",
     "shell.execute_reply.started": "2022-01-15T05:18:26.349723Z"
    }
   },
   "source": [
    "# 🔥【Python网络爬虫基础入门】数据解析三种方法re、bs4、Xpath从入门到实战。\n",
    "&emsp;&emsp;正所谓爬虫写的好，牢饭吃到饱，爬取网易云音乐、爬取B站视频想必大家都不陌生，那么我们如何去实现呢？<br>\n",
    "&emsp;&emsp;看到 AI Studio中的众多数据集，你是否有想过怎样**方便快捷**的收集自己的**数据集**呢？\n",
    "<br>&emsp;&emsp;你又是否有想过在一个人的深夜，如何收集众多的优美图片\n",
    "\n",
    "**那么就请你看完这篇项目，你一定会有所收获的（手动狗头）**\n",
    ">先看后赞，养成习惯\n",
    "\n",
    ">folk收藏，人生辉煌\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/f907ebf2dec34f81945f0479f0cc89fbc9789653df6a4d8f9af5e322e05eeb95)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、什么是网络爬虫\n",
    "百度百科给的介绍如下：\n",
    ">&emsp;&emsp;网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。\n",
    "\n",
    "&emsp;&emsp;通俗来讲，假如你需要互联网上的信息，如商品价格，图片视频资源等，但你又不想或者不能自己一个一个自己去打开网页收集，这时候你便写了一个程序，让程序按照你指定好的规则去互联网上收集信息，这便是爬虫，我们熟知的**百度，谷歌等搜索引擎背后其实也是一个巨大的爬虫。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **爬虫合法吗？**\n",
    "&emsp;&emsp;🎯可能很多小伙伴都会又这个疑问，首先爬虫是一门技术，技术应该是中立的，合不合法其实取决于你使用目的，是由爬虫背后的人来决定的，而不是爬虫来决定的。另外我们爬取信息的时候也可以稍微**‘克制’**一下，能拿到自己想要的信息就够了，没必要对着人家一直撸，看看我们的12306都被逼成啥样了🤧🤧🤧。\n",
    "&emsp;&emsp;一般来说只要不影响人家网站的正常运转，也不是出于商业目的，人家一般也就只会封下的IP，账号之类的，不至于法律风险👌。\n",
    "&emsp;&emsp;其实大部分网站都会有一个robots协议，在网站的根目录下会有个robots.txt的文件，里面写明了网站里面哪些内容可以抓取，哪些不允许。\n",
    "\n",
    "然后我们以百度搜索为例子：[https://www.baidu.com/robots.txt](https://www.baidu.com/robots.txt) \n",
    "<br>（我们只需要在网页后缀加上/robots.txt）\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/689dec2d3ae14df6a4e127e5181a60a91e9cd1dbcfd14d1db9660f25657478dd)\n",
    "<br>&emsp;&emsp;当然robots协议本身也只是一个业内的约定，是不具有法律意义的，所以遵不遵守呢也只能取决于用户本身的底线了。（懂得都懂）\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、网络爬虫能做什么\n",
    "\n",
    "&emsp;&emsp;上面说了一大堆，可能也没说清楚爬虫究竟是什么。没关系，我们举几个例子来看。\n",
    "<br>&emsp;&emsp;比如，学校经常在官网上发布一些比较重要的通知，我不想每天都花费精力去看官网，却又想当有新通知的时候，就能知道，并看到它。\n",
    "<br>&emsp;&emsp;这种时候，就需要爬虫来帮忙咯。写一个程序，让它**每半个小时或一个小时**就去访问一次官网，检查有没有新的通知，如果没有，就什么都不做，等待下次检查，如果有，就将新通知从网页中提取出来，保存，并发邮件告诉我们通知的内容，然后继续等待即可。\n",
    "<br>&emsp;&emsp;假设，最近有点闲了，想看看电影，但又不想看烂片。于是，默默打开了豆瓣，上面有电影评分嘛，还有影评。我想要获取所以评分在８分以上的电影名称、简介以及该电影的部分热评，**从中选出想看的出来**。\n",
    "<br>&emsp;&emsp;这个时候，一个小小的爬虫就能轻轻松松地从一堆电影中**找出符合要求的保存下来**，不用费神地一个个去瞅了。如果你还会自然语言处理和机器学习，那就更棒了，或许你可以直接对这些数据进行分析，让程序匹配出你感兴趣的电影来。（当然了，举例子嘛，现实生活中，显然投入和产出不成正比= =看个电影哪那么麻烦orz）\n",
    "<br>&emsp;&emsp;🎯再比如，采集京东、淘宝的商品评论信息啦，采集招聘网站的企业职位信息啦，采集微博信息啦，或者只是简单地爬一些**美女图片**啦……各种情况，采什么，看需求吧。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、开始实战\n",
    "\n",
    "&emsp;&emsp;为了方便，我使用一块一个的小案例进行演示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 爬取百度搜索\n",
    "\n",
    "### **（1）手刃小爬虫**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 首先导入我们爬虫所必须的reques库\n",
    "import requests\n",
    "#接下来我们尝试向百度搜索发起请求，我们命名了一个名为resp的response对象，从中我们就可以得到我们想要的信息\n",
    "resp = requests.get(\"https://www.baidu.com/\")\n",
    "resp.encoding = \"utf-8\"   # 调节编码格式，否则输出后会有乱码\n",
    "resp.close()   #  划重点，记得使用完后要关闭resp\n",
    "\n",
    "# 返回请求状态码，200即为请求成功\n",
    "# print(resp.status_code)\n",
    "\n",
    "# 返回页面代码\n",
    "print(resp.text)\n",
    "\n",
    "# 对于特定类型请求，如Ajax请求返回的json数据\n",
    "# print(resp.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;很棒呢！你已经成功手刃了第一个爬虫小程序👌👌👌👌👌👌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **（2）稍稍改进**\n",
    "\n",
    "&emsp;&emsp;但是当我们爬取时候如果遇到类似这样形式，说明网页已经识别出我们是一个爬虫程序了，那我们应该如何做呢？\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/b39ca5aee3904606924c7f6ae3a9615e109e4500bead4b3f9f50455488616f97)\n",
    "\n",
    "&emsp;&emsp;对于大部分网站都会需要你**表明你的身份**，我们一般正常访问网站都会附带一个请求头`headers`信息，里面包含了你的浏览器，编码等内容，网站会通过这部分信息来判断你的身份，所以我们一般写爬虫也加上一个headers。\n",
    "<br>&emsp;&emsp;那我们如何得到请求头呢，我们可以在打开浏览器之后点击**F12打开我们的开发者工具**，刷新页面我们就可以找到我们的**请求头**。\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/5f9c2fb0e1124d628947df876b5e4a7128ad06b1c6b94645a56559b4f91ce41d)这个蓝色区域便是我们要找的`User-Agent`\n",
    "\n",
    "\n",
    "<br>&emsp;&emsp;以下我们**修改**我们之前的程序。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 首先导入我们爬虫所必须的reques库\n",
    "import requests\n",
    "\n",
    "url = \"https://www.baidu.com/\"\n",
    "header = {\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.20 Safari/537.36\"\n",
    "}  # 修改成你的User-Agent\n",
    "# 那我们如何得到请求头呢？上面也给出了解释\n",
    "\n",
    "resp = requests.get(url,headers = header)\n",
    "resp.encoding = \"utf-8\" \n",
    "resp.close()   \n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 手刃百度翻译\n",
    "\n",
    "\n",
    "我们刚刚搞完百度搜索，接下来我们整百度翻译。\n",
    "<br>&emsp;&emsp;这里和上面就又有些不同了，我们引出下一个小知识点\n",
    ">&emsp;&emsp;**get 请求**：一般情况下，只从服务器获取数据下来，并不会对服务器资源产生任何影响的时候会使用get请求。\n",
    "<br>&emsp;&emsp;**post请求**：向服务器发送数据（登录）、上传文件等，会对服务器资源产生影响的时候会使用post请求。\n",
    "\n",
    "**这次针对post请求**\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/2277e57ff4ab41e1b863fcf2050ae86a10ddf05ac2624b4a9431c502c24367a2)\n",
    "\n",
    "\n",
    ">对于查询的单词都会存放在以下这个地方，我们把它放在一个字典中。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/3e19fff58ac94024baf4e9a673ef4fa081877ddff5604c0f9abfd075532e1f06)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://fanyi.baidu.com/sug\"\n",
    "\n",
    "\n",
    "# 我们这次弄的稍微高级一点\n",
    "s = input(\"请输入你要翻译的英文单词:\")\n",
    "dit = {\n",
    "    \"kw\":s\n",
    "}  #  详见上面解析\n",
    "\n",
    "#发送post请求，发送的数据必须放在字典中，通过data参数进行传递\n",
    "response = requests.post(url,data = dit)\n",
    "response.close()\n",
    "print(response.json())   #将服务器返回的内容直接处理成json()=>dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 解决模拟用户登录\n",
    ">想必上面两种已经明白思路和原理了，那我们如果碰到需要登录的，需要账户密码的那应该怎么办呢？\n",
    "\n",
    "&emsp;&emsp;我们尝试在这个小说网站进行操作[https://www.17k.com/](https://www.17k.com/)\n",
    "<br>&emsp;&emsp;很多时候等于需要登录的站点我们可能需要保持一个会话，不然每次请求都先登录一遍效率太低，所以我们要**新建一个会话**保持他的正常使用。`requests.session()`\n",
    "\n",
    "\n",
    "\n",
    "&emsp;&emsp;我们的目的是得到**个人账户中书架上的内容**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 输入账号密码进行登录 ----> 得到cookie\n",
    "# 带着cookie 去请求到书架url ----> 书架上的内容\n",
    "\n",
    "# 必须得把上面的两个操作连起来\n",
    "# 我们新建一个会话session进行请求，以此来保证在这个过程中的cookie不会丢失\n",
    "\n",
    "import requests\n",
    "\n",
    "# 会话\n",
    "session = requests.session()\n",
    "# 1.登录\n",
    "data = {\n",
    "    \"loginName\": \"15735528336\",  #  修改成你的账号\n",
    "    \"password\": \"ybh.0927\"       #  修改成你的密码\n",
    "}  # 此处为了方便演示，我暂时使用我的账号\n",
    "\n",
    "url = \"https://passport.17k.com/ck/user/login\"  #此处网址为用户页\n",
    "\n",
    "resp = session.post(url, data=data)\n",
    "resp.encoding = \"utf-8\"\n",
    "# print(resp.cookies)  #  看看cookies\n",
    "resp.close()\n",
    "# 2.拿书架上的数据\n",
    "# 刚才的那个session中是有cookie的\n",
    "re = session.get(\"https://user.17k.com/ck/author/shelf?page=1&appKey=2406394919\")   #  此处网址为个人书架\n",
    "print(re.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 爬取豆瓣（1.0）\n",
    "\n",
    "&emsp;&emsp;经过刚刚的一系列学习想必，大家也有了初步的一些认识<br>\n",
    "&emsp;&emsp;接下来我们尝试在这个网址进行我们所需要的**数据提取**[https://movie.douban.com/typerank?type_name=%E5%89%A7%E6%83%85&type=11&interval_id=100:90&action=](https://movie.douban.com/typerank?type_name=%E5%89%A7%E6%83%85&type=11&interval_id=100:90&action=)\n",
    "&emsp;&emsp;我们要找的url在这个地方\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/aa68ce01296141fb9e9d99fbb584fd5af45ab324ab254b53ae557d23b6e10219)\n",
    "所以url = [https://movie.douban.com/j/chart/top_list?type=11&interval_id=100%3A90&action=&start=0&limit=20](https://movie.douban.com/j/chart/top_list?type=11&interval_id=100%3A90&action=&start=0&limit=20)\n",
    "\n",
    "&emsp;&emsp;🌈我们可以看到这个网站相当长那么我们如何表示这些**参数**呢？这时我们就要借助这一个**参数**进行表示了：`params`🔥\n",
    "<br>那么我们的**参数**在哪里寻找呢？我们可以看一下下面这个图\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/21b7be3ed18f4373b8bc23354ebb3cd767a2abfcbf104a24a33a56e0788661fb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://movie.douban.com/j/chart/top_list\"  #  后面的参数部分我们借助下面的param进行替代\n",
    "\n",
    "# 重新封装参数(此处就为上面后面的参数部分，我们多看几个类型也可以发现下面的参数会不断变化)\n",
    "param = {\n",
    "    \"type\": \"11\",\n",
    "    \"interval_id\": \"100:90\",\n",
    "    \"action\": \"\",\n",
    "    \"start\": 0,\n",
    "    \"limit\": 20\n",
    "}\n",
    "# 依旧使用我们的请求头\n",
    "header = {\n",
    "    \"User-Agent\" :\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.20 Safari/537.36\"\n",
    "}  # 修改成你的user-agent，具体方法详细见上文介绍\n",
    "response = requests.get(url = url,params=param,headers = header)\n",
    "response.close()\n",
    "# print(response.request.url)\n",
    "\n",
    "# print(response.request.headers)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、数据解析\n",
    "## 1. 概述\n",
    "&emsp;&emsp;经过刚才一系列的操作我们基本上掌握了抓取整个⽹⻚的基本技能，但是可能在⼤\n",
    "多数情况下, 我们并不需要整个⽹⻚的内容, 只是需要那么⼀⼩部分.\n",
    "<br>&emsp;&emsp;那么我们应该怎么办呢? 这就涉及到了**数据提取**的问题\n",
    ">我们主要从以下三种方式进行入手：re正则化、bs4、XPath\n",
    "\n",
    "## 2.re正则表达式\n",
    "\n",
    "&emsp;&emsp;Regular Expression, 正则表达式,这是⼀种使⽤表达式的⽅式对字符串\n",
    "进⾏匹配的语法规则.我们抓取到的⽹⻚源代码本质上就是⼀个**超⻓的字符串**, 想从⾥⾯提取内容，⽤正则再合适不过了。\n",
    "<br>&emsp;&emsp;具体的匹配格式如下：\n",
    "\n",
    "**常用元字符：**\n",
    "* .   匹配除换行符以外的任意字符\n",
    "* \\w  匹配字母或数字或下划线\n",
    "* \\s  匹配任意的空白符\n",
    "* \\d  匹配数字\n",
    "* \\n  匹配一个换行符豆\n",
    "* \\t  匹配一个制表符\n",
    "\n",
    "* ^   匹配字符串的开始\n",
    "* $   匹配字符串的结尾\n",
    "* \\W  匹配非字母或数字或下划线\n",
    "* \\D  匹配非数字\n",
    "* \\S  匹配非空白符alb\n",
    "* a|b 匹配字符a或字符b\n",
    "* ()  匹配括号内的表达式，也表示一个组\n",
    "\n",
    "* [...]   匹配字符组中的字符\n",
    "* [^...]  匹配除了字符组中字符的所有字符\n",
    "\n",
    "\n",
    "**量词：**\n",
    "* *重复零次或更多次\n",
    "* +重复一次或更多次\n",
    "* ?重复零次或一次\n",
    "\n",
    "* {n}     重复n次\n",
    "* {n,}    重复n次或更多次\n",
    "* {n,m}   重复n到m次\n",
    "\n",
    "**贪婪匹配和惰性匹配：**\n",
    "* .*  贪婪匹配\n",
    "* .*?  惰性匹配 (最常用)\n",
    "\n",
    "&emsp;&emsp;具体的使用方法与使用效果我们可以看一下这个文章：[https://blog.csdn.net/weixin_41779359/article/details/86234058](https://blog.csdn.net/weixin_41779359/article/details/86234058)\n",
    "&emsp;&emsp;相信大家也会有所收获的！🌈🌈🌈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）爬取豆瓣（2.0）re正则表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 拿到页面源代码  requests\n",
    "\n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = \"https://movie.douban.com/top250\"\n",
    "\n",
    "header = {\n",
    "    \"user-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.20 Safari/537.36\"\n",
    "} # 修改成你的user-agent，具体方法详细见上文介绍\n",
    "response = requests.get(url,headers = header)\n",
    "page_content = response.text\n",
    "response.close()\n",
    "# 通过re来提取有效信息  re\n",
    "# 其中的.*?边是我们去替代网页源代码中的部分\n",
    "obj = re.compile(r'<li>.*?<span class=\"title\">(?P<name>.*?)'\n",
    "                 r'</span>.*?<br>(?P<time>.*?)&nbsp.*?'\n",
    "                 r'<span class=\"rating_num\" property=\"v:average\">(?P<score>.*?)'\n",
    "                 r'</span>.*?<span>(?P<num>.*?)</span>',re.S) # (?P<名字>.*?)这里与下文输出的地方对应\n",
    "#开始匹配\n",
    "result = obj.finditer(page_content) \n",
    "for i in result:\n",
    "    print(i.group(\"name\"))  # 输出名字\n",
    "    print(i.group(\"time\").strip())  # 输出电影年份\n",
    "    print(i.group(\"score\"))  # 输出电影评分\n",
    "    print(i.group(\"num\"))  # 输出电影评价人数\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯保存csv文件格式\n",
    "&emsp;&emsp;🔥**划重点！如何将我们爬取的数据输出为CSV格式的文件呢？**<br>\n",
    "&emsp;&emsp;我们还是以上面的那个为例子，我们看一下如何保存为csv格式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import csv\n",
    "\n",
    "url = \"https://movie.douban.com/top250\"\n",
    "\n",
    "header = {\n",
    "    \"user-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.20 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(url,headers = header)\n",
    "page_content = response.text\n",
    "response.close()\n",
    "obj = re.compile(r'<li>.*?<span class=\"title\">(?P<name>.*?)'\n",
    "                 r'</span>.*?<br>(?P<time>.*?)&nbsp.*?'\n",
    "                 r'<span class=\"rating_num\" property=\"v:average\">(?P<score>.*?)'\n",
    "                 r'</span>.*?<span>(?P<num>.*?)</span>',re.S)\n",
    "result = obj.finditer(page_content)\n",
    "# 保存文件\n",
    "f = open(\"豆瓣data.csv\",mode = 'w',encoding=\"utf-8\",newline=\"\")\n",
    "csvwrite = csv.writer(f)\n",
    "for i in result:\n",
    "    dic = i.groupdict()\n",
    "    dic['time'] = dic['time'].strip()\n",
    "    csvwrite.writerow(dic.values())\n",
    "f.close() # 注意关闭文件\n",
    "print(\"over!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们可以看到文件列表已经有我们输出的样子了（豆瓣data.csv）![](https://ai-studio-static-online.cdn.bcebos.com/23b5486c9c5e483fb9d7bd3f3521af860905fa0cad6e482cb41668bf11dc7819)\n",
    "&emsp;&emsp;这样是不是你就可以建立自己的数据集了呢！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. bs4简介\n",
    "&emsp;&emsp;**Beautiful Soup** 是一个可以从`HTML`或`XML`文件中提取数据的Python库，提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。\n",
    "<br>&emsp;&emsp;详细的bs4的使用方法可以见这篇文章[https://zhuanlan.zhihu.com/p/70957053](https://zhuanlan.zhihu.com/p/70957053)\n",
    "\n",
    "\n",
    "### (1)使用bs4爬取优美图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.拿到主页面的源代码. 然后提取到子页面的链接地址, href\n",
    "# 2.通过href拿到子页面的内容. 从子页面中找到图片的下载地址 img -> src\n",
    "# 3.下载图片\n",
    "import requests\n",
    "!pip install bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "url = \"https://www.umei.cc/bizhitupian/weimeibizhi/\"\n",
    "resp = requests.get(url)\n",
    "# 下面处理乱码，因为我们在输出页面时候看到的都不是中文，而是一堆乱码，所以我妈要更改编码格式\n",
    "resp.encoding = 'utf-8'  \n",
    "resp.close()\n",
    "# print(resp.text)\n",
    "# 把源代码交给bs\n",
    "main_page = BeautifulSoup(resp.text, \"html.parser\")\n",
    "# find(标签, 属性=值)\n",
    "# find_all(标签, 属性=值)\n",
    "# 因为下面这个class属性在python中是关键字所以我们要加一个\"_\"\n",
    "alist = main_page.find(\"div\", class_=\"TypeList\").find_all(\"a\") \n",
    "# print(alist)\n",
    "for a in alist:\n",
    "    # 因为我们把第二个网页中的url输出后发现少了前缀，所以要进行拼接\n",
    "    href = url + a.get('href')[24:]  \n",
    "    # print(href)\n",
    "    child_page_resp = requests.get(href)\n",
    "    child_page_resp.encoding = 'utf-8'\n",
    "    child_page_text = child_page_resp.text\n",
    "    # 从子页面中拿到图片的下载路径\n",
    "    child_page = BeautifulSoup(child_page_text, \"html.parser\")\n",
    "    child_alist = child_page.find('div', class_=\"ImageBody\")\n",
    "    img = child_alist.find(\"img\")\n",
    "    # print(img.get('src'))\n",
    "    src = img.get('src')\n",
    "    # 下载图片\n",
    "    img_resp = requests.get(src)\n",
    "    # img_resp.content  # 这里拿到的是字节\n",
    "    img_name = src.split(\"/\")[-1]  # 拿到url中的最后一个/以后的内容\n",
    "    with open(\"data/\" + img_name, mode=\"wb\") as f:\n",
    "        f.write(img_resp.content)  # 图片内容写入文件\n",
    "\n",
    "    print(\"over!!!\", img_name)\n",
    "    time.sleep(1)  # 使每次爬取图片时候沉睡一秒，以防封掉ip\n",
    "\n",
    "print(\"all over!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接着我们就可以打开我们的**data文件夹**，我们可以看到我们爬取下来的优美图片了！![](https://ai-studio-static-online.cdn.bcebos.com/8aa787ac13134a6ea14493c1b1af494b91a2be7e20fd43d0911b930d25330821)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XPath简介\n",
    "&emsp;&emsp;**XPath**即为XML路径语言（XML Path Language），它是一种用来确定XML文档中某部分位置的语言。\n",
    "<br>\n",
    "&emsp;&emsp;Xpath的常用规则如下：<br>\n",
    ">. ：选取当前结点<br>\n",
    ".. ：选取当前结点的父节点<br>\n",
    "/ ：从当前结点开始选取直接子节点<br>\n",
    "// ：从当前结点开始选取子节点<br>\n",
    "@ ：选取属性<br>\n",
    "nodename ：选取此节点的所有子节点\n",
    "\n",
    "&emsp;&emsp;具体的操作我们可以看一下这一篇文章[https://zhuanlan.zhihu.com/p/159611891](https://zhuanlan.zhihu.com/p/159611891)\n",
    "<br>接下来我们用一个小案例入个门："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install lxml  # 安装lxml库\n",
    "from lxml import etree\n",
    "\n",
    "tree = etree.parse(\"Xpath测试.html\")  # 此案例已经上传到目录\n",
    "\n",
    "# result = tree.xpath('/html/body/ul/li/a/text()')\n",
    "# result = tree.xpath('/html/body/ul/li[1]/a/text()') # xpath的顺序是从1开始的\n",
    "# result = tree.xpath('/html/body/ol/li/a[@href=\"dapao\"]/text()') # @xxx = xxx属性的筛选\n",
    "# 上面几种我们也可以自行测试，动手尝试\n",
    "\n",
    "# print(result)\n",
    "\n",
    "list = tree.xpath('/html/body/ol/li')\n",
    "for li in list:\n",
    "    # 从每一个li中提取到文字信息\n",
    "    result = li.xpath(\"./a/text()\")  # 在li中继续去寻找，相对查找\n",
    "    print(result)\n",
    "    result2 = li.xpath(\"./a/@href\")  # 拿到属性值：@属性\n",
    "    print(result2)\n",
    "\n",
    "print(tree.xpath(\"/html/body/ul/li/a/@href\"))\n",
    "print(tree.xpath(\"/html/body/div[1]/text()\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）爬取猪八戒网\n",
    "&emsp;&emsp;因为XPath我们需要不停的去**寻找标签**，所以在使用XPath时候我们需要灵活运用**F12开发者工具**使用**元素选择工具**定位我们要找的位置，会使我们的操作变得相对简单一些。\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/3f6d0bb4693d41e78c177e233ff069dbb3d7436d313a454dbf2275238765c1b5)\n",
    "<br><br>这是我们爬取的网址：[https://taiyuan.zbj.com/search/f/?kw=saas](https://taiyuan.zbj.com/search/f/?kw=saas)\n",
    "\n",
    ">&emsp;&emsp;我们逐层定位的时候一定要细心，以防我们定位错位置\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/bf5757517fc74ebe9d58c637625880f7293060680b7148909b064e956d8e58f2)\n",
    "\n",
    ">&emsp;&emsp;当然我们也可以去使用**直接copy他的XPath路径**，这个地方我们在编程的时候可以尽量去选择使用相对路径，而避免使用绝对路径，以**减少**我们计算机的运算量。\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/630fb736f96e4b8aa164a99494ef6d57129b2f5597914a27a3a1e1eed4b5cfb8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "url = \"https://taiyuan.zbj.com/search/f/?kw=saas\"\n",
    "resp = requests.get(url)\n",
    "resp.close()\n",
    "# print(resp.text)\n",
    "html = etree.HTML(resp.text)\n",
    "divs = html.xpath(\"/html/body/div[6]/div/div/div[2]/div[5]/div[1]/div\")\n",
    "\n",
    "for div in divs:\n",
    "    price = div.xpath(\"./div/div/a[2]/div[2]/div[1]/span[1]/text()\")[0].strip(\"¥\")\n",
    "    title = \"saas\".join(div.xpath(\"./div/div/a[2]/div[2]/div[2]/p/text()\"))\n",
    "    com_name = \"\".join(div.xpath(\"./div/div/a[1]/div[1]/p/text()\")).strip()\n",
    "    print(title)\n",
    "    print(price)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结与升华\n",
    "&emsp;&emsp;未完待续，之后还会更新的！！！\n",
    ">1.这不仅仅是一篇教学文档，同样也是我在学习爬虫时的一些经验与总结，希望可以对大家起到一些帮助。\n",
    "\n",
    ">2.本篇文档还未完结，之后还会继续更新，希望大家可以继续关注\n",
    "\n",
    ">3.本项目是一个面向小白的教学，如果有不完善的地方，我会慢慢改进的。\n",
    "\n",
    ">4.同时也感谢b站大佬的视频分享，让我对于写出这篇文章有了一些帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 个人简介\n",
    ">2020级 数据科学与大数据技术专业 本科生 姚博豪\n",
    "\n",
    ">本人是一名在深度学习路上奔跑的小白。也希望我踩过坑的一些经验，可以对大家有一些帮助。\n",
    "\n",
    ">百度飞桨领航团团长\n",
    "\n",
    ">互联网＋省级银奖，中国高校计算机大赛西北赛区二等奖、华为ICT省级三等奖等\n",
    "\n",
    ">[姚菜菜的主页](https://aistudio.baidu.com/aistudio/personalcenter/thirdview/755022)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
